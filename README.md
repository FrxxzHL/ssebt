# Synthetic Source-Enhanced Back-Translation (SSE-BT)
## Introduction

Obtaining bilingual data is always harder than monolingual ones, so researchers have long exploited how to enhance machine translation performances using monolingual data. One of the most widely-used strategy is Back Translation (BT).

Admittedly, BT leads to great improvements in terms of translation quality, however, it suffer from a same problem, i.e. translationese, because the source-side texts are generated by a mt model which impeding further quality imporement. The root cause, as (Rileyet al., 2020) state, is the unavailability of natural to-natural translations. 

As shown in Figure 1, take Engilish→German training data as an example. It consists both bilingual and synthetic BT corpus. The bilingual data contains human translations from English to German and vice versa. We denote the two types of translation as EN<sub>nature</sub> → DE<sub>ht</sub> and EN<sub>ht</sub> → DE<sub>nature</sub>. The synthetic corpus generated by the BT is denoted as EN<sub>mt</sub> → DE<sub>nature</sub>. For English sentences of which style is close to mt or ht, the translation result may be closer to DE<sub>nature</sub>. However, if the input is EN<sub>nautre</sub>, the translation result can hardly reach the level of nature.

Based on previous analysis, we put forward a hypothesis: if we can mitigate translationese in source-side text generated during back-translation and make the style more natural, can we further enhance machine translation performance.

Freitag et al. (2019) train Automatic Post-Editing (APE) models using the Round-Trip Translation (RTT) strategy, making translation outputs more natural. Inspired by this method, we train a similar model to modify synthetic sourceside texts generated by back-translation, in hope of making the style more natural. Then we add the enhanced BT corpus to train a translation model. We call this method as Synthetic Source-Enhanced (SSE) Back-Translation.

As a result, there is still a large gap between synthetic data and real parallel corpora.

To address the two issues, we devise an SSE model to improve the quality of generated source sentences, as well as transfer the sentence style to a more natural one. Our experiments demonstrate that SSE model can further enhance machine translation quality.

![image](https://github.com/huawei-tsc/SSE/blob/main/sse.PNG)

## Experiment Results

### WMT14 EN-DE

Bilingual: EN-DE 4.5M  Monolingual: DE 24M

|                | orig-en   | orig-de   | WMT14     |
| -------------- | --------- | --------- | --------- |
| base           | 28.08     | 26.07     | 27.42     |
| + BT                 | 27.27     | 33.43     | 30.29     |
| &nbsp; + SSE          | 29.36     | 34.66     | 31.98     |
| &nbsp; + Sampling SSE | 29.30     | 35.40     | 32.35     |
| + Sampling BT  | 28.35     | **35.74** | 31.75     |
| &nbsp; + SSE          | **29.31** | 35.70     | **32.42** |
| &nbsp; + Sampling SSE | 29.03     | 35.72     | 32.12     |

## Pre-train Model

| model        | Description                  | data                          | arch            | download |
| ----------- | ---------------------------- | ----------------------------- | --------------- | -------- |
| EN          | wmt news en sse              | wmt 2021 NewsCrawl Random 30M | tranformer-big | comming  |
| EN-FR-DE-RU | wmt en fr de multiligual sse | wmt news                      | tranformer-big | comming  |
| SSE-100     | big multiligual sse ?        |                               |                 |          |

## How To Use

#### EN SSE

1. Data Processing

   a. Tokenize using mosesdecoder： https://github.com/moses-smt/mosesdecoder

   ```shell
   perl mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l en < bt.en > token.en
   ```

   b. Generate subwords  using BPE： https://github.com/rsennrich/subword-nmt

   ```shell
   subword-nmt apply-bpe -c wmt2014.en-de.codefile < token.en > bpe.en
   ```

2. Enhance BT data quality using SSE model

   ```python
   # Load pretrained sse model
   src_lang = "rtt"  # the mt en
   target_lang = en  # the natural en
   en2de_pre_trained = TransformerModel.from_pretrained(
       model_dir_path,
       checkpoint_file=checkpoint_file,
       data_name_or_path=model_dir_path,
       source_lang=source_lang,
       target_lang=target_lang,
       device_id=0
   )
   en2de_pre_trained.cuda()
   
   # For sampling 
   en2de_pre_trained.translate(input_a_batch, beam=1, sampling=True, sampling_topk=10)
   # For Beam search
   en2de_pre_trained.translate(input_a_batch, beam=2)
   ```

