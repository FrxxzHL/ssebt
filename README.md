# Synthetic Source-Enhanced Back-Translation (SSE-BT)
## Introduction

Obtaining bilingual data is always harder than monolingual ones, so researchers have long exploited how to enhance machine translation performances using monolingual data. One of the most widely-used strategy is Back Translation (BT).

Admittedly, BT leads to great improvements in terms of translation quality, however, it suffer from a same problem, i.e. translationese, because the source-side texts are generated by a mt model which impeding further quality imporement. The root cause, as (Rileyet al., 2020) state, is the unavailability of natural to-natural translations. 

![image](https://github.com/FrxxzHL/ssebt/blob/main/mt-ht-nature.PNG)

As shown in above figure, take Engilish→German training data as an example. It consists both bilingual and synthetic BT corpus. The bilingual data contains human translations from English to German and vice versa. We denote the two types of translation as EN<sub>nature</sub> → DE<sub>ht</sub> and EN<sub>ht</sub> → DE<sub>nature</sub>. The synthetic corpus generated by the BT is denoted as EN<sub>mt</sub> → DE<sub>nature</sub>. For English sentences of which style is close to mt or ht, the translation result may be closer to DE<sub>nature</sub>. However, if the input is EN<sub>nautre</sub>, the translation result can hardly reach the level of nature.

Based on previous analysis, we put forward a hypothesis: if we can mitigate translationese in source-side text generated during back-translation and make the style more natural, can we further enhance machine translation performance.

Freitag et al. (2019) train Automatic Post-Editing (APE) models using the Round-Trip Translation (RTT) strategy, making translation outputs more natural. Inspired by this method, we train a similar model to modify synthetic sourceside texts generated by back-translation, in hope of making the style more natural. Then we add the enhanced BT corpus to train a translation model. We call this method as Synthetic Source-Enhanced (SSE) Back-Translation. Our experiments demonstrate that SSE model can further enhance machine translation quality.

![image](https://github.com/FrxxzHL/ssebt/blob/main/sse-bt.PNG)

## Experiment Results

### SSE Model
We use 24M cleaned English monolingual data to train the SSE model. During training, we calculate the similarity between each pair of MT and original sentence, and then filter out about 10% pairs that differ the most.
The sse model is traning fellow:
1. Translate the EN monolingual data to DE<sub>mt</sub>.
2. Translate the DE<sub>mt</sub> data to EN<sub>mt</sub>.
3. Filter 10% pairs of <EN<sub>mt</sub>, EN> which differ the most. 
4. Traning a typical transformer-based seq2seq model using pairs of <EN<sub>mt</sub>, EN><sub>filtered</sub>

### WMT18 EN-DE

Bilingual: EN-DE 5.2M  Monolingual: DE 226M

<table align="center">
   <tr>
      <td ></td>
      <td colspan="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014</td>
      <td colspan="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015</td>
      <td colspan="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016</td>
      <td colspan="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017</td>
      <td colspan="3">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;average</td>
   </tr>
   <tr>
      <td></td>
      <td>All</td>
      <td>O</td>
      <td>R</td>
      <td>All</td>
      <td>O</td>
      <td>R</td>
      <td>All</td>
      <td>O</td>
      <td>R</td>
      <td>All</td>
      <td>O</td>
      <td>R</td>
      <td>All</td>
      <td>O</td>
      <td>R</td>
   </tr>
   <tr>
      <td>Bitext</td>
      <td>28.2</td>
      <td>28.2</td>
      <td>28.3</td>
      <td>30.8</td>
      <td>32.8</td>
      <td>26.1</td>
      <td>34.6</td>
      <td>37.6</td>
      <td>29.8</td>
      <td>28.7</td>
      <td>31.1</td>
      <td>25.2</td>
      <td>30.6</td>
      <td>32.4</td>
      <td>27.4</td>
   </tr>
   <tr>
      <td>Beam BT</td>
      <td>28.8</td>
      <td>23.7</td>
      <td>35.2</td>
      <td>28.9</td>
      <td>27.6</td>
      <td>30.6</td>
      <td>33.2</td>
      <td>28.7</td>
      <td>39.3</td>
      <td>29.2</td>
      <td>26.1</td>
      <td>32.3</td>
      <td>30</td>
      <td>26.5</td>
      <td>34.4</td>
   </tr>
   <tr>
      <td>&nbsp;&nbsp;+SSE</td>
      <td>32.1</td>
      <td>28.5</td>
      <td>36.7</td>
      <td>32.3</td>
      <td>31.9</td>
      <td>32.5</td>
      <td>36.4</td>
      <td>33.7</td>
      <td>40</td>
      <td>31.5</td>
      <td>28.9</td>
      <td>33.8</td>
      <td>33.1</td>
      <td>30.8</td>
      <td>35.8</td>
   </tr>
   <tr>
      <td>Sampling BT</td>
      <td>33.7</td>
      <td>28.8</td>
      <td>39.4</td>
      <td>33.8</td>
      <td>32.4</td>
      <td>35.7</td>
      <td>36.5</td>
      <td>32.9</td>
      <td>41.7</td>
      <td>32</td>
      <td>28.3</td>
      <td>36.4</td>
      <td>34</td>
      <td>30.6</td>
      <td>38.3</td>
   </tr>
   <tr>
      <td>&nbsp;&nbsp;+SSE</td>
      <td>33.8</td>
      <td>29.3</td>
      <td>38.9</td>
      <td>33.5</td>
      <td>32</td>
      <td>35.9</td>
      <td>36.4</td>
      <td>32.6</td>
      <td>41.7</td>
      <td>31.7</td>
      <td>28.1</td>
      <td>36</td>
      <td>33.9</td>
      <td>30.5</td>
      <td>38.1</td>
   </tr>
   <tr>
      <td>Noised BT</td>
      <td>32.5</td>
      <td>29.8</td>
      <td>36.1</td>
      <td>33.4</td>
      <td>34.3</td>
      <td>31.5</td>
      <td>38.4</td>
      <td>38</td>
      <td>38.4</td>
      <td>31.9</td>
      <td>31.5</td>
      <td>31.8</td>
      <td>34.1</td>
      <td>33.4</td>
      <td>34.5</td>
   </tr>
   <tr>
      <td>&nbsp;&nbsp;+SSE</td>
      <td>33</td>
      <td>30.4</td>
      <td>36.4</td>
      <td>34.2</td>
      <td>34.9</td>
      <td>32.8</td>
      <td>38.8</td>
      <td>38.1</td>
      <td>39.4</td>
      <td>32.6</td>
      <td>32</td>
      <td>32.7</td>
      <td>34.7</td>
      <td>33.9</td>
      <td>35.3</td>
   </tr>
   <tr>
      <td>Tagged BT</td>
      <td>32.7</td>
      <td>30</td>
      <td>36</td>
      <td>34.1</td>
      <td>34.4</td>
      <td>32.3</td>
      <td>38.7</td>
      <td>38.6</td>
      <td>38.7</td>
      <td>32.9</td>
      <td>32.6</td>
      <td>32.3</td>
      <td>34.6</td>
      <td>33.9</td>
      <td>34.8</td>
   </tr>
   <tr>
      <td>&nbsp;&nbsp;+SSE</td>
      <td>33</td>
      <td>30.6</td>
      <td>36.1</td>
      <td>34.5</td>
      <td>35.6</td>
      <td>31.8</td>
      <td>39.3</td>
      <td>39.8</td>
      <td>38.3</td>
      <td>32.9</td>
      <td>32.7</td>
      <td>32.2</td>
      <td>34.9</td>
      <td>34.7</td>
      <td>34.6</td>
   </tr>
   <tr>
      <td>FT</td>
      <td>28.7</td>
      <td>29.2</td>
      <td>28.1</td>
      <td>31.5</td>
      <td>33.8</td>
      <td>26.1</td>
      <td>35.6</td>
      <td>38.8</td>
      <td>30.3</td>
      <td>29.5</td>
      <td>32.5</td>
      <td>25.3</td>
      <td>31.3</td>
      <td>33.6</td>
      <td>27.5</td>
   </tr>
   <tr>
      <td>&nbsp;&nbsp;+Beam BT</td>
      <td>32.3</td>
      <td>30.1</td>
      <td>35.2</td>
      <td>33.7</td>
      <td>35.1</td>
      <td>30.5</td>
      <td>39.8</td>
      <td>40.4</td>
      <td>38.5</td>
      <td>32.7</td>
      <td>32.6</td>
      <td>32</td>
      <td>34.6</td>
      <td>34.6</td>
      <td>34.1</td>
   </tr>
   <tr>
      <td>&nbsp;&nbsp;&nbsp;&nbsp;+SSE</td>
      <td>32.7</td>
      <td>30</td>
      <td>36</td>
      <td>34.2</td>
      <td>35.1</td>
      <td>31.9</td>
      <td>40.5</td>
      <td>40.5</td>
      <td>40</td>
      <td>33.4</td>
      <td>33.2</td>
      <td>33</td>
      <td>35.2</td>
      <td>34.7</td>
      <td>35.2</td>
   </tr>
   <tr>
      <td></td>
   </tr>
</table>


## Using SSE to Boost your NMT

#### EN SSE

1. Data Processing

   a. Tokenize using mosesdecoder： https://github.com/moses-smt/mosesdecoder

   ```shell
   perl mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l en < bt.en > token.en
   ```

   b. Generate subwords  using BPE： https://github.com/rsennrich/subword-nmt

   ```shell
   subword-nmt apply-bpe -c wmt2014.en-de.codefile < token.en > bpe.en
   ```

2. Enhance BT data quality using SSE model

   ```python
   # Load pretrained sse model
   src_lang = "rtt"  # the mt en
   target_lang = en  # the natural en
   en2de_pre_trained = TransformerModel.from_pretrained(
       model_dir_path,
       checkpoint_file=checkpoint_file,
       data_name_or_path=model_dir_path,
       source_lang=source_lang,
       target_lang=target_lang,
       device_id=0
   )
   en2de_pre_trained.cuda()
   
   # For sampling 
   en2de_pre_trained.translate(input_a_batch, beam=1, sampling=True, sampling_topk=10)
   # For Beam search
   en2de_pre_trained.translate(input_a_batch, beam=2)
   ```

